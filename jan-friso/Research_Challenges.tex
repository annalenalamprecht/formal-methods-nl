%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[sigplan,10pt,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,10pt,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,10pt,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[sigplan,10pt]{acmart}\settopmatter{}

\usepackage{paralist}

%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmConference[PL'17]{ACM SIGPLAN Conference on Programming Languages}{January 01--03, 2017}{New York, NY, USA}
\acmYear{2017}
\acmISBN{} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2017}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%\citestyle{acmauthoryear}  %% For author/year citations
%\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption


\begin{document}

%% Title information
\title[Short Title]{Title}         %% [Short Title] is optional;
\title[Short Title]{Formal modelling and analysis} 
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%\titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
%\subtitle{Subtitle}                     %% \subtitle is optional
\subtitle{The road to affordable, high quality (control) software}
%\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{Jan Friso Groote, Bas Luttik, Julien Schmaltz, Erik de Vink, Wieger Wesselink, Tim Willemse\vspace{2ex}}
\email{{J.F.Groote, S.P.Luttik, J.Schmaltz, E.P.d.Vink, J.W.Wesselink, T.A.C.Willemse}@tue.nl}
%\email{j.f.groote@tue.nl}          %% \email is recommended
%\author{Bas Luttik}
%\email{s.p.luttik@tue.nl}          %% \email is recommended
%\author{Erik de Vink}
%\email{e.p.d.vink@tue.nl}          %% \email is recommended
%\author{Tim Willemse}
%\email{t.a.c.willemse@tue.nl}          %% \email is recommended
\affiliation{Department of Mathematics and Computer Science\\ 
  \institution{Eindhoven University of Technology}           %% \institution is required
}


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
The research interest of the Formal Systems Analysis (FSA) research group at Eindhoven University lies
in the development of techniques to efficiently develop software that is guaranteed
to work as expected. Our tools and methods are starting to find widespread use in the embedded systems industry. 
We identify the success factors and indicate what we believe are the most important research directions. 
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
%\keywords{keyword1, keyword2, keyword3}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}
Can we make software that works most of the time? The answer is yes, and it is all around us. 
But a trickier question is whether we can make software that \textit{always} works as expected and that is still affordable.
The answer to that is quite open, and it is one of the fundamental quests of computer science. In the Formal Systems Analysis
group at Eindhoven University we believe that the solution to this quest can only be found through the use of
mathematical analysis techniques assisted by appropriate methods and tools. 

We can safely say that throughout the years we made good progress towards an answer. 
We have analysed the actual software of numerous large scale 
projects varying from train and bridge controllers \cite{GKRSW13,GKV95}, control software for wafer steppers \cite{Jonk16}
and X-ray scanners \cite{DBLP:journals/sttt/OsaiweranSHGR16} and even the 
detector control software of the CMS and ATLAS projects at CERN ~\cite{HwongKKLW13}. In almost all cases we could identify flaws, have them repaired
and we could always show that the resulting software was performing according to given correctness requirements. 
This success led to the uptake of the mCRL2
toolset as the verification backend of the Verum toolset. Verum is currently a major supplier for tools to design 
reliable embedded software. 
Analysis of the use of Verum's methods show that they lead to a tenfold reduction of errors and a speedup of development of 
a factor three \cite{DBLP:journals/sttt/OsaiweranSHGR16}. 

Below we identify the factors that made these projects successful and how they lead the way of further research.
The successful methods need to be strengthened and they have to made available to larger groups of
developers at an even further reduced cost. 


\subsection{Formal Model-Driven Engineering}

All projects where we successfully applied model checking to increase the quality of the
code were using some form of Domain Specific Language (DSL). All DSLs that we encountered
essentially consisted of a finite automata language with inputs and outputs. Examples
are safety programmable logical controllers (PLCs), finite automata at CERN and ASD and Dezyne
at Verum \cite{DBLP:conf/fm/Broadfoot05}. 

The effectiveness of DSLs has three causes. Firstly, programs written in a DSL are much more concise
than programs written in a general programming language. Secondly, DSLs restrict the expressivity of
programmers, which means that programs written in DSLs are far more comprehensible 
by other programmers. Finally, DSLs are easier to analyse and verify by formal means, meaning that
their quality can be made much higher than that of regular programs. This last aspect is probably the most important
of these three. Formal verification is an effective way the let the software attain its desired 
quality, avoiding expensive testing and continuous redesign. 

In all cases we wrote translators from DSLs to the most appropriate verification framework
(mCRL2 \cite{DBLP:books/mit/GrooteM2014} or SAT) to perform the verification. Formal verification of systems written in general 
purpose languages require manual translation to a formal setting. As such software
is complex and often badly documented, its translation is generally unsatisfactory and inefficient,
unsuitable to become part of an effective workflow. 

Model-Driven engineering with behavioural analysis can be made more effective in the following ways:
\begin{compactitem}
\item 
More effective means to transform programs written in a DSL into a verification framework.
It is not efficient to build verification toolsets for each DSL from scratch. 
It should be possible to define the translations abstractly, after which the necessary 
transformation framework is automatically generated.
\item 
An effective theory to denote and reason about the meaning of DSL programs. This theory should
also allow to assess the correctness of the translations to verification formalisms. 

\item 
An understanding of the do's and don'ts within the design of DSLs such that they are suitable for their domains,
allow verification and do not become too expressive hampering understandability of programs written in it. 
As an example the DSLs ASD and Dezyne do not allow to write
programs that manipulate input data, to avoid a state space explosion that could hamper verifiability. 

\end{compactitem}

\subsection{Coordination Architecture}
Another important observation behind most of the effective verifications is that
the architecture of the system has a strong influence
on verifiability \cite{GrooteKoutersOsaiweran}. At CERN we are speaking about systems of up to 60,000 cooperating components
in a strict tree structure. At ASML we verified a system with 250 components with on average more than
1000 `rule cases' each, of which the coordination architecture
could be transformed into a tree \cite{Jonk16}. 

But it is unlikely that tree like software architectures would fit all applications. We need to identify
which software structures are more amenable to verification, and which verification techniques are most
suitable for those cases.

We then need to change the attitude to system design. When setting up a system, the  
software architecture that is chosen must not only be suitable for the purpose of the
software, but also need to  fit the verification needs.

\subsection{A Unified Analysis Framework}

The diverse, and relatively separate visions
and solutions for analysing the behaviour of systems within various
specialised problem domains have led to a situation in which results
that are obtained in one domain are not easily transfered to other
domains.  There is a serious risk that this will inhibit the progress
we can achieve in the near future.  

What is needed is to identify a formalism that unifies existing
behavioural analysis techniques.  Such a formalism should be
independent of the specification languages used to describe the
systems, but, more importantly, the formalism should be sufficiently
powerful to unify all the techniques developed in the separate
specialised verification disciplines.

A serious candidate is the formalism of \emph{parameterised
Boolean equation systems}~\cite{GrooteW05}. This formalism is firmly rooted in
mathematical fixpoint theory and logic, and admits an elegant game
theoretical interpretation.  Model checking in 
mCRL2 is done via a translation to PBESs.  Moreover, the techniques
developed for PBESs can be studied in their own right, leading to
new insights into existing theories. A beautiful example thereof
is the theory of abstraction for PBESs~\cite{CranenGWW15}, which,
inspired by abstraction theories for transition systems, substantially improves
upon it.

The PBES theory has, by now, successfully demonstrated its status as a unified
framework for analysing
data-dependent systems and real-time systems. However, more is needed.
Apart from continuing the prime research line of unifying existing specialised
techniques (\emph{e.g.}\ partial order and symmetry reduction) for data-dependent 
and real-time systems into the the theory of PBESs we need to:
\begin{compactitem}

\item define specialised theories for 
PBESs that involve (fragments of) real-valued and complex numbers; such theories are
needed to deal with continuous variables and problems stemming from \emph{e.g.}\
hybrid systems and hybrid approaches to fluid dynamics. Of particular interest
is the algorithmic study of PBESs that include such numbers;

\item extend the PBES theory to deal with quantitative analysis
problems to allow for analysing probabilistic and stochastic systems,
optimisation problems, \emph{etcetera}, and demonstrating that these extensions
are sufficiently powerful to solve the analytical problems in the
respective application areas.

\item address the fundamental open questions in the theory of PBESs;
\emph{e.g.} establishing the long standing open problem of the exact computational complexity of
solving PBESs or parity games~\cite{CaludeJKL017}; identifying a complete (as in
\emph{finite}) abstraction theory for PBESs; identifying the limits
of PBESs as a unifying framework.


\end{compactitem}
%






%% Acknowledgments
%\begin{acks}                            %% acks environment is optional
%                                        %% contents suppressed with 'anonymous'
%  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%  %% acknowledge financial support and will be used by metadata
%  %% extraction tools.
%  This material is based upon work supported by the
%  \grantsponsor{GS100000001}{National Science
%    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%  conclusions or recommendations expressed in this material are those
%  of the author and do not necessarily reflect the views of the
%  National Science Foundation.
%\end{acks}


%% Bibliography
\bibliography{Research_Challenges.bib}


%% Appendix

\end{document}

% Residues.

Model-Driven Engineering (MDE) has caused a profound shift in
activities in software engineering by gradually replacing coding
by more abstract activities such as modelling and designing
(domain-specific) languages. The industrial eagerness to adopt the
methodology is rooted in an increased productivity, enabled by the
raised abstraction level offered by the models, and the automated
code generated from such models, and a latent promise of more
reliable software. However, there is no mechanism built in MDE's
methodology that guarantees that software with the required high
quality and reliability is delivered, see \emph{e.g.}~\cite{HwongKKLW13};
assessing such attributes is still the hallmark of formal methods,
which MDE has largely failed to integrate.

While the MDE methodology primarily focusses on models as the
solutions to an engineering problem, the analysis of the involved
models is typically beyond the scope of the methodology.  \emph{Formal}
Model-Driven Engineering, a formal approach to MDE, is a natural
progression of MDE, in which not only the solution, but also the
\emph{correctness and quality guarantees} are derived from the
involved models and languages through techniques such as model
checking and automated theorem proving.

The challenges for formal methods in FMDE are more complex and
diverse than those found in the traditional software engineering
setting.  On the one hand, since models typically abstract from the
execution semantics, the models are no longer self-contained.
Instead, such generic information, which is important to establish
the quality and correctness of the engineered system, is encoded
in the construction (\emph{i.e.}, semantics) of the domain-specific
language.  Formalising the semantics in such a way that it can be
used in concert with the model to assess their quality and correctness
is not self-evident. The state-of-the-art relies on providing
translational semantics for DSLs,  the main motivation being that
this potentially allows for enabling formal verification by encoding
the DSL in a formal language. However, such semantics are often
ad-hoc, difficult to maintain, but, foremost, not rooted in formal
theory, and mainly useful for \emph{defining} the semantics, not
for \emph{analysing} it since there is no reference semantics.

On the other hand, the scale at which formal methods will be applied
in the FMDE methodology is beyond that of the traditional
software engineering setting since models, and systems of
models, are at the heart of FMDE. This means that one can no longer
rely on the insights of the lone hero applying manually constructed
abstractions to achieve the required scalability to make the
underlying analysis technology work for her application. Consequently,
what is needed is:
\begin{compactitem}
\item unified mathematical theories for (de)compositionality,
refinement and abstraction, and the associated algorithms and tools
to deal with the complexity of models that have been generated
automatically;

\item new classes of data structures and the associated algorithmics
to fully exploit the structure offered by the abstract concepts of
the DSLs.

\end{compactitem}

